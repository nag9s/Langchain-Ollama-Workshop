{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents loaded: 38\n",
      "Total document chunks: 201\n",
      "\n",
      "--- Retrieved Documents ---\n",
      "Total documents retrieved: 5\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 1:\n",
      "Score: N/A\n",
      "Source: rag-dataset/health supplements/3.health_supplements_side_effects.pdf\n",
      "\n",
      "Content:\n",
      "component of Geranium plants, e.g. as geranium extract (71). However, the presence of \n",
      "DMAA in plants has not been verified, leading to the conclusion that DMAA in supplements \n",
      "is generated by chemical synthesis (72). DMAA has further been banned as a performance \n",
      "enhancing drug by the World Anti-Doping Agency (73). One version of the weight-loss \n",
      "supplement OxyELITE Pro from USPlabs, LLC contained the compound 1,3-\n",
      "dimethylamylamine (DMAA) in addition to ingredients such as caffeine, Bauhinia p... [truncated]\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 2:\n",
      "Score: N/A\n",
      "Source: rag-dataset/health supplements/1. dietary supplements - for whom.pdf\n",
      "\n",
      "Content:\n",
      "Int. J. Environ. Res. Public Health 2021, 18, 8897\n",
      "13 of 24\n",
      "3.3. Dietary Supplements and Weight Loss\n",
      "The number of people with excessive weight continues to rise, and ﬁghting obesity\n",
      "has become one of the greatest challenges of contemporary medicine. A person wishing to\n",
      "lose weight needs to undertake several difﬁcult life-changes and practice them consistently\n",
      "(diet, physical activity, addiction-free). Meanwhile dietary supplements are presented as\n",
      "a compelling alternative to traditional methods... [truncated]\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 3:\n",
      "Score: N/A\n",
      "Source: rag-dataset/health supplements/3.health_supplements_side_effects.pdf\n",
      "\n",
      "Content:\n",
      "women consuming isoflavone supplements (59) and, given the clear evidence of \n",
      "estrogenicity, there is a likelihood of increased risk of estrogen sensitive cancers in \n",
      "consumers of these products.\n",
      "WEIGHT-LOSS, SPORTS, AND BODYBUILDING SUPPLEMENTS\n",
      "As more and more of the world population becomes overweight and obese, there is a huge \n",
      "market for weight-loss products, including dietary supplements. Among military service \n",
      "members, athletes and bodybuilders it is also common to ingest dietary sports ... [truncated]\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 4:\n",
      "Score: N/A\n",
      "Source: rag-dataset/health supplements/1. dietary supplements - for whom.pdf\n",
      "\n",
      "Content:\n",
      "yohimbine, and even amphetamine and its derivatives being found in supplements con-\n",
      "tinues to grow [105,106,127,137]. A US study revealed that 11 out of 21 supplements that\n",
      "contained the Acacia rigidula extract that were purchased on the Internet contained the iso-\n",
      "mer of amphetamine [138]. In South Korea, substances that bore a structural resemblance\n",
      "to amphetamine were found in 10 out of 110 weight loss supplements [139]. In Italy, 28%\n",
      "of the supplements purchased online contained sibutramine ... [truncated]\n",
      "--------------------------------------------------\n",
      "\n",
      "Document 5:\n",
      "Score: N/A\n",
      "Source: rag-dataset/health supplements/3.health_supplements_side_effects.pdf\n",
      "\n",
      "Content:\n",
      "(64). Yet, such supplements exist and can readily be obtained e.g. over the internet. \n",
      "Furthermore, they may be more likely to give real physiological effects desired by the \n",
      "consumer due to the pharmacological efficiency of anabolic steroids or other drugs \n",
      "incorporated in the supplements. Medical providers and toxicologists should be therefore be \n",
      "aware of symptoms elicited by these compounds.\n",
      "Body weight supplements with some documented weight-loss effects were those containing \n",
      "extracts of t... [truncated]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import warnings\n",
    "import tiktoken\n",
    "import faiss\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Document Loading Libraries\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "def load_pdf_documents(directory):\n",
    "    \"\"\"\n",
    "    Load PDF documents from a specified directory.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Path to the directory containing PDF files\n",
    "    \n",
    "    Returns:\n",
    "        list: List of loaded documents\n",
    "    \"\"\"\n",
    "    pdfs = []\n",
    "    docs = []\n",
    "    \n",
    "    # Find all PDF files in the specified directory\n",
    "    for root, _, files in os.walk(directory):\n",
    "        pdfs.extend([os.path.join(root, file) for file in files if file.endswith(\".pdf\")])\n",
    "    \n",
    "    # Load each PDF document\n",
    "    for pdf in pdfs:\n",
    "        loader = PyMuPDFLoader(pdf)\n",
    "        docs.extend(loader.load())\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def chunk_documents(docs, chunk_size=1000, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks.\n",
    "    \n",
    "    Args:\n",
    "        docs (list): List of documents to chunk\n",
    "        chunk_size (int): Size of each document chunk\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        list: List of document chunks\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return text_splitter.split_documents(docs)\n",
    "\n",
    "def create_vector_store(chunks, embedding_model='nomic-embed-text', base_url='http://localhost:11434'):\n",
    "    \"\"\"\n",
    "    Create a vector store from document chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): List of document chunks\n",
    "        embedding_model (str): Name of the embedding model\n",
    "        base_url (str): Base URL for Ollama embeddings\n",
    "    \n",
    "    Returns:\n",
    "        FAISS: Vector store with embedded documents\n",
    "    \"\"\"\n",
    "    # Initialize embeddings\n",
    "    embeddings = OllamaEmbeddings(model=embedding_model, base_url=base_url)\n",
    "    \n",
    "    # Create vector embedding\n",
    "    vector = embeddings.embed_query(\"Hello World\")\n",
    "    \n",
    "    # Create FAISS index\n",
    "    index = faiss.IndexFlatL2(len(vector))\n",
    "    vector_store = FAISS(\n",
    "        embedding_function=embeddings,\n",
    "        index=index,\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "    )\n",
    "    \n",
    "    # Add documents to vector store\n",
    "    vector_store.add_documents(documents=chunks)\n",
    "    \n",
    "    return vector_store\n",
    "def print_retrieved_docs(retrieved_docs, max_length=500):\n",
    "    \"\"\"\n",
    "    Print retrieved documents in a clean, readable format.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_docs (list): List of retrieved documents\n",
    "        max_length (int): Maximum length of content to display\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Retrieved Documents ---\")\n",
    "    print(f\"Total documents retrieved: {len(retrieved_docs)}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        print(f\"\\nDocument {i}:\")\n",
    "        print(f\"Score: {doc.metadata.get('score', 'N/A')}\")\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "        \n",
    "        # Truncate content if it's too long\n",
    "        content = doc.page_content\n",
    "        if len(content) > max_length:\n",
    "            content = content[:max_length] + \"... [truncated]\"\n",
    "        \n",
    "        print(\"\\nContent:\")\n",
    "        print(content)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Main function to orchestrate document processing and vector store creation.\n",
    "    \"\"\"\n",
    "    # Suppress warnings (optional)\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # Load PDF documents\n",
    "    docs = load_pdf_documents(\"rag-dataset/health supplements\")\n",
    "    \n",
    "    # Optional: Check document count and content\n",
    "    print(f\"Total documents loaded: {len(docs)}\")\n",
    "    \n",
    "    # Chunk documents\n",
    "    chunks = chunk_documents(docs)\n",
    "    print(f\"Total document chunks: {len(chunks)}\")\n",
    "    \n",
    "    # Optional: Tokenization check\n",
    "    # encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "    # token_lengths = [len(encoding.encode(chunk.page_content)) for chunk in chunks[:3]]\n",
    "    # print(f\"Token lengths of first 3 chunks: {token_lengths}\")\n",
    "    \n",
    "    # Create vector store\n",
    "    vector_store = create_vector_store(chunks)\n",
    "    \n",
    "    # Example retrieval\n",
    "    question = \"What nutritional supplements support muscle protein synthesis?\"\n",
    "    retrieved_docs = vector_store.search(query=question, k=5, search_type=\"similarity\")\n",
    "\n",
    "    print_retrieved_docs(retrieved_docs)\n",
    "    \n",
    "    # Optional: Save vector store\n",
    "    db_name = \"../health_supplements\"\n",
    "    vector_store.save_local(db_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
