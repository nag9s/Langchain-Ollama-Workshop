{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Retrieved Documents ===\n",
      "I'm happy to help, but there's no context provided for the question. Could you please provide more information or clarify what document we're referring to? I'll do my best to answer your question accurately.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Import FAISS and related libraries\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from uuid import uuid4\n",
    "\n",
    "# 1. Load PDF Document\n",
    "pdf_path = \"dietarySupplements.pdf\"\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# 2. Split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Adjust based on your document\n",
    "    chunk_overlap=50  # Some overlap to maintain context\n",
    ")\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# 3. Create Embeddings using Ollama\n",
    "embeddings = OllamaEmbeddings(model='nomic-embed-text', base_url='http://localhost:11434')\n",
    "\n",
    "# 4. Create FAISS Vector Store\n",
    "def create_faiss_vector_store(documents, embeddings):\n",
    "    # Create a FAISS index\n",
    "    embedding_dimension = len(embeddings.embed_query(\"test\"))\n",
    "    index = faiss.IndexFlatL2(embedding_dimension)\n",
    "    \n",
    "    # Create vector store\n",
    "    vector_store = FAISS(\n",
    "        embedding_function=embeddings,\n",
    "        index=index,\n",
    "        docstore=InMemoryDocstore({str(uuid4()): doc for doc in documents}),\n",
    "        index_to_docstore_id={}\n",
    "    )\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "# Create FAISS vector store\n",
    "vectorstore = create_faiss_vector_store(split_docs, embeddings)\n",
    "\n",
    "# 5. Create Retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 3,  # Retrieve top 3 most relevant chunks\n",
    "        \"search_type\": \"mmr\"  # Maximal Marginal Relevance\n",
    "    }\n",
    ")\n",
    "\n",
    "# 6. Create Chat Model using Ollama\n",
    "chat_model = ChatOllama(model='llama3.2:3b', base_url='http://localhost:11434')\n",
    "\n",
    "# 7. Create RAG Prompt Template\n",
    "prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert assistant. Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\")\n",
    "\n",
    "query = \"What is the main topic of this document?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(\"=== Retrieved Documents ===\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    print(\"Content:\", doc.page_content)\n",
    "    print(\"Metadata:\", doc.metadata)\n",
    "# 8. Create RAG Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 9. Example Usage\n",
    "query = \"What is the main topic of this document?\"\n",
    "response = rag_chain.invoke(query)\n",
    "print(response)\n",
    "\n",
    "# Optional: Save and load the vector store\n",
    "def save_and_load_example():\n",
    "    # Save the vector store locally\n",
    "    vectorstore.save_local(\"faiss_index\")\n",
    "    \n",
    "    # Load the vector store\n",
    "    loaded_vectorstore = FAISS.load_local(\n",
    "        \"faiss_index\", \n",
    "        embeddings, \n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    \n",
    "    # Use the loaded vector store\n",
    "    loaded_retriever = loaded_vectorstore.as_retriever()\n",
    "    return loaded_retriever\n",
    "\n",
    "# Demonstrate saving and loading\n",
    "loaded_retriever = save_and_load_example()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
