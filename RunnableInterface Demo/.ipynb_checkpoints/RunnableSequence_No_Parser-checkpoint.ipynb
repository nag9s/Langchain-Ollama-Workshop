{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e716a34-ed5b-4476-ab8c-2c3a6660a837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6caf2db-0362-41d4-8e63-871d9112e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "base_url = \"http://localhost:11434\"\n",
    "model = 'llama3.2:1b'\n",
    "# Initialize the Llama model \n",
    "llm = ChatOllama(\n",
    "    base_url=base_url,\n",
    "    model = model,\n",
    "    temperature = 0.8,\n",
    "    num_predict = 256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6649b76-ce6c-4b44-8599-0887ed01921a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LlamaCpp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RunnableSequence\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize the Llama model (make sure to replace with your actual model path)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaCpp\u001b[49m(\n\u001b[1;32m      7\u001b[0m     model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/path/to/your/llama/model.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m      9\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     10\u001b[0m     n_ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Create a prompt template for financial transaction analysis\u001b[39;00m\n\u001b[1;32m     14\u001b[0m transaction_analysis_prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Analyze the following payment transaction details:\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    - Transaction Amount: {amount}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    Provide a detailed risk assessment and potential fraud indicators.\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LlamaCpp' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "\n",
    "\n",
    "# Create a prompt template for financial transaction analysis\n",
    "transaction_analysis_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Analyze the following payment transaction details:\n",
    "    - Transaction Amount: {amount}\n",
    "    - Merchant: {merchant}\n",
    "    - Transaction Type: {transaction_type}\n",
    "\n",
    "    Provide a detailed risk assessment and potential fraud indicators.\"\"\"\n",
    ")\n",
    "\n",
    "# Create a RunnableSequence to process the transaction\n",
    "transaction_analysis_chain = (\n",
    "    transaction_analysis_prompt  # First, format the prompt\n",
    "    | llm  # Then, pass to the Llama model for analysis\n",
    "    \n",
    ")\n",
    "\n",
    "# Example usage\n",
    "def analyze_transaction(amount, merchant, transaction_type):\n",
    "    result = transaction_analysis_chain.invoke({\n",
    "        \"amount\": amount,\n",
    "        \"merchant\": merchant,\n",
    "        \"transaction_type\": transaction_type\n",
    "    })\n",
    "    return result\n",
    "\n",
    "# Demonstrate the chain\n",
    "transaction_result = analyze_transaction(\n",
    "    amount=\"$1,250.75\", \n",
    "    merchant=\"TechCorp Online Store\", \n",
    "    transaction_type=\"E-commerce Purchase\"\n",
    ")\n",
    "\n",
    "print(\"Transaction Analysis:\")\n",
    "print(transaction_result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
